[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pos-pln",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Ordem sugerida\n\nPlanejando sua Carreira para as Profissões do Futuro\nIntrodução à Lógica de Programação\nSistema Operacional Linux, Docker e Kubernetes\nGovernança de Dados\nSoft Skill-Desenvolvendo Suas Habilidades Comportamentais\nE-Gov Analytics\nMachine Learning com JavaScript e Go\nData Science e Machine Learning com Linguagem Julia\nDeep Learning Para Aplicações de Inteligência Artificial com Python e C++\nAplicações de LLM na área médica\nIA Generativa e LLM para processamento de linguagem natural\nIA Generativa e LLM para área do direito"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "js_go.html#definition",
    "href": "js_go.html#definition",
    "title": "3  Machine Learning com JavaScript e Go",
    "section": "3.1 Definition :",
    "text": "3.1 Definition :\n\nJavaScript pode ser executada no navegador ou por uma Engine, o Chrome, Opera e Edge utilizam a engine V8 e o Firefox a SpiderMonkey, já no servidor podemos utilizar o Node.Js"
  },
  {
    "objectID": "js_go.html#livros-e-links-úteis",
    "href": "js_go.html#livros-e-links-úteis",
    "title": "3  Machine Learning com JavaScript e Go",
    "section": "3.3 Livros e links úteis",
    "text": "3.3 Livros e links úteis\n\nJavaScript The Definitive Guide (David Fionagan) PDF\nEloquent JavaScript https://github.com/braziljs/eloquente-javascript?tab=readme-ov-file\nJavaScript - Mozilla\nECMAScript® 2025 Language Specification"
  },
  {
    "objectID": "js_go.html#section",
    "href": "js_go.html#section",
    "title": "3  Machine Learning com JavaScript e Go",
    "section": "3.3 ",
    "text": "3.3"
  },
  {
    "objectID": "js_go.html#para-rodar-o-projeto",
    "href": "js_go.html#para-rodar-o-projeto",
    "title": "3  Machine Learning com JavaScript e Go",
    "section": "3.2 Para rodar o projeto",
    "text": "3.2 Para rodar o projeto\n\n3.2.1 Instalar Node.js , exemplo on (linux mint)\n\nDownload bin nodejs.org\nCriar os diretórios\n\nsudo mkdir -p /usr/local/lib/nodejs\nsudo tar -xJvf node-v14.18.0-linux-x64.tar.xz -C /usr/local/lib/nodejs\n\nEditar o .profile, abrir xed ~/.profile & e adicionar :\n\n# Nodejs\nVERSION=vXXXXX\nDISTRO=linux-x64\nexport PATH=/usr/local/lib/nodejs/node-$VERSION-$DISTRO/bin:$PATH\n\n\nVerificar versões :\n\nnode -v\nnpm version\nnpx -v\n\n\n3.2.2 To Run :\nCom estes comandos vamos testar em DEV e fazer o build para acessar http://localhost:4500\n\nPara testar\n\nnpm install\nnpm run dev\n\nPara deploy\n\nnpm install\nnpm run build\nnpm run start"
  },
  {
    "objectID": "dl1.html#intro",
    "href": "dl1.html#intro",
    "title": "4  Deep Learning Para Aplicações de Inteligência Artificial com Python e C++",
    "section": "4.1 Intro",
    "text": "4.1 Intro\n\n4.1.1 História :\n\n1943 : Conceito de um modelo de rede neural\n1958 : Perceptron (primeira arquitetura de rede neural)\n1969 : Pesquisadores publicam que o perceptron não funciona\n : **Inverno da IA**\n1986 : Backpropagation\n1989 : RNN - revulução na PLN\n1997 : LSTM, variante da RNN\n2012 : AlexNet vence ImageNet\n2017 : Arquitetura Transformer, paper Attention is All You Need\n2020 : LLM (GPT-3)\n2022 : OpenAI publica ChatGPT\nHoje : LLM - IA Generativa\n\n\n\n4.1.2 Arquiteturas :\n\nDNN : (Redes neurais desamente conectadas)\nCNN : (Redes neurais convolucionais)\nRNN : (Redes Neurais Recorrentes)\nAutoencoders\nGANs : (Redes Adversariais Generativas)\nRedes Siamesas\nModelos de Capsule\nModelos de Atenção e Transformes\n\n\n\n4.1.3 Conceitos\n\nActivation function : Introduzem a não linearidade o que permite que a rede modelo funções complexas e regularização, alguns tipos de funções de ativação Sigmoid, Tanh, ReLU ( Leaky ReLU, Parametric ReLU, GenLU)\nOverfitting : Quando o modelo aprende demais sobre os dados e não consegue generalizar\nUnderfitting : Quando o modelo é muito simples. não se ajusta aos dados de treino, ou seja , o modelo não aprendeuos padrões dos dados\nRegularizaçáo: L1 (penaliza soma absoluta) e L2 (penaliza a soma dos quadrados) são tecnicas de regressão linear e logística\nDropout : certos neurônios são desligados aleatoriamente em cada interação\nEarly Stopping : interrompe o treinamento assim que o desempenho começam a degradar.\nLoss function : quantifica o quão bem as previsões de um modelo se alinham com os valores reais observados. A escolha da função depende do tipo de problema a ser resolvido:\n\nRegressão : MSE, MAE\nClassificação: Emtropia cruzada ou log loss, Hinge Loss\nModelos Generativos GANs: Gradiente descendente\n\nTools and Frameworks\nFrameworks Python : PyTorch, TensorFlow, MxNet, JAX, ONNX\nBibliotecas C++ : Armadillo, MLPack\nBackpropagation\n\nO modelo irá fazer a primeira passada de calculo Forward Pass e calcular o erro.\nDepois disso o algoritmo de Backpropagation irá através de cálculos de derivadas reduzir o erro do modelo (Loss), isso é feito alterando os pesos novamente com objetivo de reduzir o erro final.\nNovo peso = Peso anterior - Derivada * Learing Rate\n\n\n\n\n4.1.4 Mais detalhes sobre Algumas redes e arquiteturas\n\nCNN : Utilizada para detecção de objetos e lidar com imagens\nRNN : Utilizadas para linguagem natural ou series temporais, capaz de manter um estado de memória. Temos algumas variações LSTM (Long Short-term memory) e GRU (Gated Recurrent Units)\nRedes Neurais Generativas : Redes que permitem a geração de novos dados semelhantes aos dados que foram treinados, uma arquitetura de Redes generativas é a GANs (Redes Adversariais Generativas) que são duas redes treinadas simultaneamente (O gerador e o discriminador)\n\nGerador : Produz dados novos a partir de ruído aleatório\nDiscriminador : Tenta distinguir entre amostras geradas (fake) e dados reais\nO treinamento contiua até que o gerador se torne suficientemente bom para produzir dados que o discriminador não consiga diferenciar entre reais ou fake.\nOutro tipo de rede neural generativa é o Modelo Autorregressivo como PixelRNN, utilizado para gerar imagens ou música,\nTemos também Redes Geradoras de Momento Variacional (Variacional Autoencoders VAEs) : A ideia é aprender a distribuição latente dos dados de entrada e em sequida gerar novos dados\n\nMecanismos de Atenção e Transformadores: focam e partes específicas da entrada\n\n\n\n4.1.5 Transfer Learning e Modelos Pré-treinados\n\nTransfer Learning é uma técnica onde um modelo desenvolvida para uma tarefa é reutilizado como ponto de partida para outra tarefa relacionada, pode ser utilizado como estratégia de inicialização de pesos\nModelos pré-treinados são modelos ja treinados em grandes bases de dados:\n\nVisão Computacional : VGGNet, ResNet, Inspectino\nPLN : BERT, GPT, Llama, T5\n\n\n\n\n4.1.6 Ótimização e Regularização\n\nOtimização : processo de ajustar os parâmetros (pesos) do modelo com o objetivo de minimizar a função de perda e com isso encontrar o conjunto ótimo de parametros que resulte na melhor performance do modelo, algoritmos utilizados :\n\nGradiente descendente\nGradiente descendente Estocastico (SGD)\nMomentum\nAdam\nBatch Normalization\n\nRegularização conjunto de técnicas que visam impedir o overfitting, algumas técnicas :\n\nL1 e L2\nDropout (Desativa neuronios durante o treinamento)\nEarly Stopping (Interronpe o treinamento assim que a performance piora)\n\n\n\n\n4.1.7 Hugging Face\nGrante primeira tentativa de criar LLM (Large Language Model)\n\nBERT Doc\nBERT Model card\n\n\n\n\n4.1.8 Explorando o ChatGPT\nExplore o [ChatGPT]}(https://chatgpt.com/)\n\nModeli simples\nModelo complexo\n\n\n\n4.1.9 Configuração do Amiente de Desenvolvimento\n\nAnaconda\nGoogle Colab\nVisual Studio Code"
  },
  {
    "objectID": "dl1.html#projecto-1",
    "href": "dl1.html#projecto-1",
    "title": "4  Deep Learning Para Aplicações de Inteligência Artificial com Python e C++",
    "section": "4.3 Projecto 1",
    "text": "4.3 Projecto 1\nAttention Is All You Need:\nExemplo simples para visualizar os modulos e a camada de atenção:\n\nJupyter Notebook Project 1\nNa arquitetura transformes o mecanismo de atenção do tipo (Scaled dot-product) utiliza três componentes :\n\nQ_(Query)_ : representa parte que estamos interessados, por exemplo : Em PLN poderia ser a frase que estamos tentando traduzir. Em um modelo transformer para cada posição uma query é gerada e são usadas para pontar a qualidade da entrada.\nK_(Key)_ : Usada para pontar a entrada e comparada com a query para determinar o grau de atenção, essa comparação resulta em um conjunto de pontuação que indica a relevancia de cada parta da entrada para representar a query\n\n\nK e Q determina onde o modelo deve focar\n\n\nV_(Value)_ : contém a info real que queremos extrair, compoe a saida do mecanismo de atenção, cada value é associado a uma key.\n\n\nO mecanismo de atenção calcula um conjunto de pontuações e aplica softmax para obter pesos de atenção e usa esses pesos para ponderar os values, criando uma saída ."
  },
  {
    "objectID": "dl1.html#referencia",
    "href": "dl1.html#referencia",
    "title": "4  Deep Learning Para Aplicações de Inteligência Artificial com Python e C++",
    "section": "4.4 Referencia :",
    "text": "4.4 Referencia :\n\nDeep Learning book\nAlgoritmo Backpropagatio\nWhat’s the Backward-Forward FLOP Ratio for Neural Networks?\nForward e Backward Pass\n[A Comprehensive Guide to the Backpropagation Algorithm in Neural Networks](https://neptune.ai/blog/backpropagation-algorithm-in-neural-networks-guide]\nWhat is forward and backward propagation in Deep Learning?\n\n*[A Deep Dive Into the Transformer Architecture –The Development of Transformer Models](https://www.exxactcorp.com/blog/Deep-Learning/a-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models\n\n3 Alternativas Para Usar LLMs\nTransformers: is attention all we need in finance?\nAttention Is All You Need"
  },
  {
    "objectID": "dl1.html#fundamentos",
    "href": "dl1.html#fundamentos",
    "title": "4  Deep Learning Para Aplicações de Inteligência Artificial com Python e C++",
    "section": "4.2 Fundamentos",
    "text": "4.2 Fundamentos\n\nPerceptron : algoritmo de aprendizagem supervisionada para classificação binária, resolve problemas linearmente separáveis\n\n\n\n\nPerceptron\n\n\nOs pesos é justamente o que o modelo aprende durante o treinamento.\n\nMLP (Multi layer perceptron)\nCriação do algoritmo de backpropagation que permite que as redes de múltiplas camadas ajustassem os pesos de forma eficaz e possibilitou a resolução de problemas não lineares.\nMLPs são compostas por uma camada de entrada, uma ou mais camadas ocultas e uma camada de saída\n\n\n\n\nMLP\n\n\n\n4.2.1 How backpropagation algorithm works:\n\nForward Pass: During the forward pass, input data is fed into the neural network, and the network’s output is computed layer by layer. Each neuron computes a weighted sum of its inputs, applies an activation function to the result, and passes the output to the neurons in the next layer.\nLoss Computation: After the forward pass, the network’s output is compared to the true target values, and a loss function is computed to measure the discrepancy between the predicted output and the actual output.\nBackward Pass (Gradient Calculation): In the backward pass, the gradients of the loss function with respect to the network’s parameters (weights and biases) are computed using the chain rule of calculus. The gradients represent the rate of change of the loss function with respect to each parameter and provide information about how to adjust the parameters to decrease the loss.\nParameter update: Once the gradients have been computed, the network’s parameters are updated in the opposite direction of the gradients in order to minimize the loss function. This update is typically performed using an optimization algorithm such as stochastic gradient descent (SGD), that we discussed earlier.\nIterative Process: Steps 1-4 are repeated iteratively for a fixed number of epochs or until convergence criteria are met. During each iteration, the network’s parameters are adjusted based on the gradients computed in the backward pass, gradually reducing the loss and improving the model’s performance."
  }
]